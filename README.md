# Gradient Boosting Regressor (From Scratch)

---

## ğŸ“Œ Project Overview
This project implements a **Gradient Boosting Machine (GBM)** for regression tasks completely from first principles using Python and NumPy. It avoids using high-level boosting libraries (like XGBoost or LightGBM) to demonstrate a deep understanding of the underlying algorithms.

The model is trained and evaluated on the **Boston Housing Dataset**, using Decision Trees as weak learners to minimize Mean Squared Error (MSE) via gradient descent in function space.

## ğŸš€ Key Features
* **Custom Implementation:** Core boosting logic (`fit`, `predict`) implemented manually in the `GradientBoostingRegressorScratch` class.
* **Hyperparameter Tuning:** Supports configuration of:
    * `n_estimators` (Number of boosting stages)
    * `learning_rate` (Shrinkage parameter to prevent overfitting)
    * `max_depth` (Complexity of individual weak learners)
* **Loss Function:** Optimization based on **Squared Error Loss** ($L = \frac{1}{2}(y - \hat{y})^2$).
* **Robust Data Pipeline:** Handles the deprecated Boston Housing dataset by fetching directly from the CMU StatLib repository.

## ğŸ› ï¸ Technologies Used
* **Language:** Python 3.x
* **Core Logic:** NumPy (Matrix operations), Pandas (Data handling)
* **Base Learner:** Scikit-learn (`DecisionTreeRegressor` used *only* as the weak learner)
* **Visualization:** Matplotlib (Training curves and residual plots)

## ğŸ“‚ Project Structure
```text
Gradient-Boosting-Machine/
â”œâ”€â”€ gbm_model.py          # Core class library containing the GBM algorithm
â”œâ”€â”€ train_eval.py         # Script to load data, train model, and generate plots
â”œâ”€â”€ README.md             # Project documentation
â”œâ”€â”€ 1_Training_Loss_Curve.png  # (Generated) Loss minimization visualization
â”œâ”€â”€ 2_Actual_vs_Predicted.png  # (Generated) Prediction scatter plot
â”œâ”€â”€ 3_Residuals_Distribution.png # (Generated) Error distribution analysis
â””â”€â”€ 4_LR_Comparison.png   # (Generated) Hyperparameter impact analysis

```
---

## âš™ï¸ Installation & Usage

### 1. Prerequisites

```bash
pip install numpy pandas matplotlib scikit-learn

```

### 2. Running the Training & Evaluation

Execute the main script to train the model and generate performance reports:

```bash
python train_eval.py

```

### 3. Using the Model in Your Code

You can import the class and use it just like a Scikit-learn estimator:

```python
from gbm_model import GradientBoostingRegressorScratch

# Initialize
model = GradientBoostingRegressorScratch(
    n_estimators=200, 
    learning_rate=0.1, 
    max_depth=3
)

# Train
model.fit(X_train, y_train)

# Predict
predictions = model.predict(X_test)

```
---

## ğŸ“Š Performance Results
On the held-out test set (20% split), the model achieves excellent convergence:

* **Test RMSE:** 2.4525 (Root Mean Squared Error)
* **Test RÂ²:** 0.9180 (Coefficient of Determination)
* **Train RMSE:** 0.8274

### Visualization Checkpoints

The script automatically generates the following insights:

1. **Training Loss Curve:** Verifies that the MSE decreases with each boosting iteration.
2. **Residual Analysis:** Confirms errors are normally distributed (validating regression assumptions).
3. **Learning Rate Comparison:** Demonstrates the trade-off between convergence speed and stability.

---
